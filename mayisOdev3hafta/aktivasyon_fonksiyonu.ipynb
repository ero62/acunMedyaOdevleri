{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ade83ed6-3758-4e4a-b1cd-660272fbad54",
   "metadata": {},
   "source": [
    "# aktivasyon fonksiyonu \n",
    "## aktivasyon fonksiyonu nedir \n",
    "    Aktivasyon fonksiyonları nöronların toplam fonksiyonun da üretilen çıktıların nasıl değişimden geçmesi gerekiğini belirler. Genelikle kullanılan aktivasyon fonksiyonu lineer olmayan fonksiyolardır. Aktivasyon fonksiyonu doğrusal olmadığı sürece, sadece birkaç nöron kullanılarak bile karmaşık problemler çözülebilir.  \n",
    "## aktivasyon fonksiyonlarında olması gerekn özelikler \n",
    "    1- doğrusal olmama\n",
    "aktivasyon fonksiyonları daha karmaşık problemlere çözüm bulabilmek için doğrusal olmamalıdır.\n",
    "    \n",
    "    2- türevlenebilirlik\n",
    " Aktivasyon fonksiyonları sürekli olmalı ve bununla birlikte birinci dereceden türevi alınabilmelidir.\n",
    " \n",
    "    3- aralık\n",
    "Daha efektif bir model eğitimi için aktivasyon fonksiyonlarının değerinin sonsuza gitmesini değil, belirli aralıklar içinde olmasını isteriz.\n",
    "\n",
    "    4- monotonluk\n",
    "Aktivasyon fonksiyonlarının monoton olması minimum veya maksimum noktalarının olacağı anlamına gelir ve bu da öğrenmenin olacağının göstergesidir.\n",
    "     \n",
    "    5- orjine göre yakınsalık\n",
    "Aktivasyon fonksiyonları orijine göre yakınsak olduğunda YSA modellerinin başlangıç ağırlıkları rastgele küçük değerler olarak atandığında öğrenme gerçekleşir. Aksi takdirde daha farklı yöntemler ile modelin ağırlıkları atanmalıdır.\n",
    "\n",
    "\n",
    "## aktivasyon fonksiyonu türleri \n",
    "    1. Sigmoid Fonksiyonu\n",
    "\n",
    "Geçmişte oldukça popüler olan bu fonksiyon aldığı değerleri 0 ve 1 arasına hapseder. Yüksek bir değer geldiğinde 1'e yakın olurken düşük bir değer geldiğinde 0'a yakın olur. Dolayısıyla 1'i ve 0'ı geçen herhangi bir değer gözlemlenmez. Sigmoid fonksiyonunun dezavantajları ise birden fazladır. Gradient ölümünün fazla olması, 0 odaklı bir fonksiyon olmaması ve exp() hesaplamalarının oldukça yavaş olması bunlara örnek olarak gösterilebilir.\n",
    "\n",
    "    2. tanh (Hiperbolik Tanjant) Fonksiyonu\n",
    "\n",
    "Hiperbolik tanjant kısaca “tanh”n sigmoid’e benzerliği ile bilinir. İkisinde de sıkıştırma bulunur ancak tanh gelen değerleri [-1,1] arasına hapseder. Sigmoid fonksiyonuna göre daha iyi sonuçlar alınabilir çünkü 0 odaklıdır. Özellikle LSTM ve GRU gibi sinir ağlarında yaygın olarak kullanılan bir aktivasyon fonksiyonudur. Ancak gradient ölümleri hala devam etmektedir.\n",
    "\n",
    "    3.ReLu Fonksiyonu\n",
    "\n",
    "Günümüzde en sık kullanılan ve popülerliğini koruyan aktivasyon fonksiyonudur. İlk olarak 2012 yılında kullanıldı ve günümüze kadar popülerliği giderek arttı. Ayrıca ReLu fonksiyonu biyolojik nörona benzerliğiyle bilinir. Çalışma mantığı olarak gelen değerlerin pozitif mi negatif mi olduğuna bakar sonrasında eğer gelen değer negatif bir değerse işlem sonucunu 0 verir. Ancak gelen değer pozitifse herhangi bir sıkıştırma ya da değiştirme işlemi uygulamaz olduğu gibi geçer. Bilgisayarlar tanh ve sigmoid’de karmaşık hesaplamalar yaparken ReLu’da yalnızca pozitiflik negatiflik durumuna bakar. Bu nedenle bilgisayar bu denklemi 6 kat daha hızlı hesaplar. Kısacası hızlıdır. Ancak muhteşem bir fonksiyon diyemeyiz çünkü bu fonksiyonda 0 odaklı olmadığından bazı nöronlar (%40'a kadar) ölebiliyor.\n",
    "\n",
    "    4. Leaky ReLu Fonksiyonu\n",
    "\n",
    "ReLu fonksiyonlarında dezavantajlardan biri olarak nöronların ölümünden bahsetmiştik. Hatta bu ölümler bazen %40'a kadar çıkabilmektedir. İşte Leaky ReLu fonksiyonu tam burada devreye giriyor. Ölü nöron sonunu ortadan kaldırmak için geliştirilmiş bir fonksiyondur diyebiliriz. Negatif bir değer geldiğinde çok küçük bir sayı döndürür ve bu sayede nöronların ölmesinin önüne geçer. Hesaplama sayısının artmasına rağmen diğer fonksiyonlara göre oldukça hızlı çalışır. Son olarak ReLu ‘nun sahip olduğu tüm avantajları barındırır.\n",
    "\n",
    "    5. Exponential Linear Unit (ELU)\n",
    "\n",
    "Bu fonksiyonda orta nokta 0 olduğundan ölü nöron sorunun önüne geçebilir. Exponential yani e^x cinsinden hesaplama yaptığı için oldukça yavaştır. Ancak ReLu’nun avantajlarına sahiptir.\n",
    "\n",
    "    6. Maxout Fonksiyonu\n",
    "\n",
    "Kullanlan bir diğer aktivasyon fonksiyonu ise Maxout fonksiyonudur. ReLu ve Leaky ReLu’yu genelleştiren bir fonksiyondur ancak parametre sayısı 2 katına çıktığından ReLu’ ya göre oldukça yavaş çalışır. Avantajı ise ReLu’da olduğu gibi nöron ölümü gerçekleşmez. Son olarak maxout fonksiyonunun herhangi bir grafiksel gösterimi yoktur.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1a8a56-739a-417c-87a4-2a3438bbe595",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
